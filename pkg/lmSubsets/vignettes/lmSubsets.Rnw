%====================================================================%
% PREAMBLE                                                           %
%====================================================================%

\documentclass[nojss]{jss}
\shortcites{hamill:b_am_meteorol_soc:13}


%--------------------------------------------------------------------%
% vignette                                                           %
%--------------------------------------------------------------------%

% \VignetteIndexEntry{lmSubsets: Efficient Computation of Variable-Subsets Linear Regression in R}
% \VignetteKeywords{linear regression, model selection, variable selection, best-subset regression, R}
% \VignettePackage{lmSubsets}
% \VignetteDepends{stats,graphics,memisc}


%--------------------------------------------------------------------%
% Sweave options                                                     %
%--------------------------------------------------------------------%

%% pdflatex:  set 'eps=FALSE'
\SweaveOpts{engine=R,eps=FALSE,keep.source=TRUE}


%--------------------------------------------------------------------%
% packages                                                           %
%--------------------------------------------------------------------%

\usepackage{thumbpdf,lmodern}
\usepackage{booktabs,dcolumn}
\usepackage{pdfcomment}
\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-tree}
\usepackage{auto-pst-pdf}


%--------------------------------------------------------------------%
% commands                                                           %
%--------------------------------------------------------------------%

\newcommand{\Rset}{\mathbb{R}}
\newcommand{\rss}{\mathrm{RSS}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\card}[1]{\lvert{#1}\rvert}
\newcommand{\norm}[1]{\|{#1}\|_2}
\newcommand{\SSS}{\proglang{S}3}
\newcommand{\R}{\proglang{R}}
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\class}[1]{\dquote{\code{#1}}}
\newcommand{\drop}{\mathsf{drop}}
%
\newcommand{\TODO}[1]{\texttt{TODO: #1}}


%--------------------------------------------------------------------%
% front matter                                                       %
%--------------------------------------------------------------------%

\author{%
  Marc Hofmann\\\TODO{affiliation}
  \And Cristian Gatu\\\TODO{affiliation}
  \And Erricos J. Kontoghiorghes\\Birbeck College
  \AND Ana Colubi\\University of Oviedo
  \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Marc Hofmann, Cristian Gatu, Erricos J. Kontoghiorghes, Ana Colubi, Achim Zeileis}

\title{\pkg{lmSubsets}: Efficient Computation of Variable-Subsets Linear Regression in \proglang{R}}
\Plaintitle{lmSubsets: Efficient Computation of Variable-Subsets Linear Regression in R}

\Abstract{ 
  \TODO{write abstract}
}

\Keywords{linear regression, model selection, variable selection, best subset regression, \proglang{R}}
\Plainkeywords{linear regression, model selection, variable selection, best subset regression, R}

\Address{
  Marc Hofmann\\
  \TODO{address}
}

%====================================================================%
% DOCUMENT                                                           %
%====================================================================%

\begin{document}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("lmSubsets")
data("AirPollution")
@


%--------------------------------------------------------------------%
% section:  Introduction                                             %
%--------------------------------------------------------------------%

\section{Introduction}
\label{sec:intro}

\TODO{Some comments and references to applications might be useful here.}

An important problem in statistical modeling is that of
subset selection regression or, equivalently, of finding the best
regression equation~\citep{hastie:01}.  Given a set of possible
variables to be included in the regression, the problem is to select a
subset that optimizes some statistical criterion.  The latter
originates in the estimation of the corresponding
submodel~\citep{miller:02}.  Consider the standard regression model
%
\begin{equation}
  \label{eq:olm}
  %
  y=X\beta+\epsilon,%\quad\epsilon\sim(0,\sigma^2I_M),
\end{equation}
%
where $y\in\Rset^M$ is the output variable, $X\in\Rset^{M\times N}$ is
the regressor matrix of full column rank, $\beta\in\Rset^N$ is
the coefficient vector, and $\epsilon\in\Rset^M$ is the noise vector.
% with mean $0$ and variance-covariance matrix $\sigma^2I_M$.
The ordinary least squares (OLS) estimator of $\beta$ is the solution of
%
\begin{equation}
  \hat{\beta}_{\text{OLS}}=\argmin_{\beta}\rss(\beta)\text{,}
\end{equation}
%
where the residual sum of squares (RSS) of $\beta$ is given by
%
\begin{equation}
  \rss(\beta)=\norm{y-X\beta}^2\text{.}
\end{equation}
%
That is, $\hat{\beta}_{\text{OLS}}$ minimizes the norm of the residual
vector.  The computation of the RSS does not require to explicitly
determine $\beta$; it can be done by means of orthogonal matrix
decomposition methods and is numerically stable.
  
Let $V=\{1,\ldots,N\}$ denote the set of all possible variables.  A
subset model (or submodel) is denoted by $S$, $S\subseteq V$.  Given a
criterion function $f$, the {\em best-subset selection problem}
consists in solving
%
\begin{equation}
  \label{eq:best_subset}
  %
  S^*=\argmin_{S\subseteq V}f(S)\text{.}
\end{equation}
%
Here, the value $f(S)=F(n,\rho)$ is seen as a function of the number
of selected variables $n=\card{S}$ and of $\rho=\rss(S)$, the RSS of
the OLS estimator of the submodel $S$.  Furthermore, it is assumed
that $f(S)$ is monotonic with respect to $\rss(S)$ for fixed $n$, that
is
%
\begin{equation}
  \label{eq:f:monotonicity}
  %
  \rss(S_1)\leq\rss(S_2)\Rightarrow f(S_1)\leq f(S_2)\text{,}
  \quad\card{S_1}=\card{S_2}\text{.}
\end{equation}

Common selection criteria observe this structure, such as the AIC
family given by the formula
%
\begin{equation}
  \label{eq:aic}
  %
  \text{AIC}_k=M+M\log2\pi+M\log(\text{RSS}/M)+k(n+1)\text{,}
\end{equation}
%
where the scalar $k$ represents a \emph{penalty per parameter}
($k>0$).  The standard AIC and BIC criterion are obtained for $k=2$
and $k=\log M$, respectively~\citep{miller:02}.  It follows
that~\eqref{eq:best_subset} is equivalent to
%
\begin{equation*}
  S^*=S^*_\nu\text{,}\quad\text{where}\quad
  \nu=\argmin_{n}f(S^*_n)
\end{equation*}
%
and
%
\begin{equation}
  \label{eq:all_subsets}
  %
  S^*_n=\argmin_{\card{S}=n}\rss(S)
  \quad\text{for}\quad n=1,\dots,N\text{.}
\end{equation}
%
Finding the solution to~\eqref{eq:all_subsets} is called the
\emph{all-subsets selection problem}.  Thus,
solving~\eqref{eq:best_subset} can be seen as an indirect, two-stage
procedure:
%
\begin{enumerate}
\item For each subset size $n$, find the subset $S^*_n$
  ($\card{S^*_n}=n$) with the smallest RSS.
\item Compute $f(S^*_n)$ for all $n$, and determine $\nu$ such that
  $f(S^*_\nu)$ is minimal.
\end{enumerate}
%
Note that solving~\eqref{eq:best_subset} directly can allow a
computational strategy to optimize for a specific selection criterion,
thus lowering the compuational cost.  On the other hand, solving
\eqref{eq:all_subsets} has the advantage that all $N$ submodels are
available and the computationally cheap second stage
can be performed for different criterion functions (e.g., AIC and BIC)
or using more elaborate statistical inference.

\TODO{The subsequent paragraph should connect better to the previous
paragraph. It would be good to have some general discussion about
pros and cons of exact vs.\ approximate algorithms. And then we
can discuss the implementation of the different algorithms in R.}

Algorithms for subset regression that do exact search but avoid
exhaustive fitting of all models can be found in the \proglang{R}
package~\pkg{leaps}~\citep{leaps} based on \cite{miller:02}.
Exhaustive search has been considered also within the context of
generalized linear models and summarized in the \pkg{bestglm}
package~\citep{bestglm}.
%
Alternative approximate solutions based on simulated annealing were
introduced in the \pkg{subselect} package~\citep{subselect} based
on~\cite{duarte_silva:j_multivariate_anal:01}.  Furthermore,
approximate solutions via genetic algorithms for generalized linear
models and beyond have been implemented in
\pkg{glmulti}~\citep{calcagno:j_stat_softw:10} and \pkg{kofnGA}
\citep{wolters:j_stat_softw:15}.  Regularized estimation of parametric
models with automatic variable selection is performed by lasso or
elastic net estimation for generalized linear
models~\citep{friedman:j_stat_softw:10}.

Here, the \pkg{lmSubsets} package for exact variable-subset regression
is presented. It embeds functions for solving both the {\em
  best-subset}~\eqref{eq:best_subset} and the {\em
  all-subsets}~\eqref{eq:all_subsets} problems.  It implements the
algorithms presented by \cite{gatu:j_comput_graph_stat:06} and
\cite{hofmann:comput_stat_data_an:07}.  A branch-and-bound strategy is
employed to reduce the size of the search space.  The package further
proposes non-exhaustive methods that compute approximate solutions
very quickly while giving guarantees on the quality of the results.
The core or the package is written in \proglang{C++}.  The package is
available for the \proglang{R} system for statistical
computing~\citep{R} from The Comprehensive \proglang{R} Archive Network at
\url{https://CRAN.R-project.org/package=lmSubsets}.

\TODO{The remainder of this manuscript is organized \dots}


%--------------------------------------------------------------------%
% section:  Computational strategies                                 %
%--------------------------------------------------------------------%

\section{Computational strategies}
\label{sec:comput}

Before discussing the implementation in \pkg{lmSubsets}, the underlying
methods and algorithms are briefly reviewed. Emphasis is given to the
general idea and strategy whereas for all theory and algorithmic details
we refer to \cite{gatu:j_comput_graph_stat:06} and \cite{hofmann:comput_stat_data_an:07},
respectively.

The linear regression model~\eqref{eq:olm} has $2^N$ possible subset
models which can be efficiently organized in a regression tree.  A
dropping column algorithm (DCA) was devised as a straight-forward
approach to solve the all-subsets selection
problem~\eqref{eq:all_subsets}.  The DCA evaluates all possible
variable subsets by traversing a regression tree consisting of
$2^{N-1}$
nodes~\citep{gatu:parallel_comput:03,smith:comput_stat_data_an:89}.

Each node $(S,k)$ of the tree corresponds to a subset
$S=\{s_1,\ldots,s_n\}$ of $n$ variables and an index $k$
($k=0,\ldots,n-1$).  The root node $(V,0)$ corresponds to the full set
of variables.  For every visited node $(S,k)$, the RSS of the $n-k$
subleading models corresponding to the subsets
$\{s_1,\ldots,s_{k+1}\}, \ldots, \{s_1,\ldots,s_n\}$ are reported.
Child nodes are generated by dropping (deleting) a variable:
%
\begin{equation*}
  \drop(S,j)=(S\setminus\{s_j\},j-1)\text{,}
  \quad j=k+1,\ldots,n-1\text{.}
\end{equation*}
%
Numerically, this is equivalent to downdating an orthogonal matrix
decomposition after a column has been deleted
\citep{golub:96,kontoghiorghes:00,smith:comput_stat_data_an:89}.
Givens rotations are employed to efficiently move from one node to
another.  The DCA maintains a subset table $r$ with $N$ entries; entry
$r_n$ contains the RSS and the variable subset of the current-best
submodel of size
$n$~\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07}.
Figure~\ref{fig:dca_tree} illustrates a regression tree for $N=5$
variables.  The index $k$ is symbolized by a bullet ($\bullet$).  The
subleading models are listed in each node.

\begin{figure}[t!]
 \renewcommand{\arraystretch}{0.75}
\def\node#1#2{\TR{\psframebox[framearc=0.5]{\begin{tabular}{c}$\mathsf{#1}$\\{\scriptsize$\mathsf{#2}$}\end{tabular}}}}
  \begin{center}
    \pstree[levelsep=12ex,treesep=2.5ex,edge=\ncline]{%
      \node{{\bullet}12345}{1,12,123,1234,12345}}{%
      \pstree{%
        \node{{\bullet}2345}{2,23,234,2345}}{%
        \pstree{%
          \node{{\bullet}345}{3,34,345}}{%
          \pstree{%
            \node{{\bullet}45}{4,45}}{%
            \node{{\bullet}5}{5}}
          \node{3{\bullet}5}{35}}
        \pstree{%
          \node{2{\bullet}45}{24,245}}{%
          \node{2{\bullet}5}{25}}
        \node{23{\bullet}5}{235}}
      \pstree{%
        \node{1{\bullet}345}{13,134,1345}}{%
        \pstree{%
          \node{1{\bullet}45}{14,145}}{%
          \node{1{\bullet}5}{15}}
        \node{13{\bullet}5}{135}}
      \pstree{%
        \node{12{\bullet}45}{124,1245}}{%
        \node{12{\bullet}5}{125}}
      \node{123{\bullet}5}{1235}}
\end{center}
\caption{All-subsets regression tree (with subleading models) for
  $N=5$ variables.}
\label{fig:dca_tree}
\end{figure}

The DCA is computationally demanding, with a theoretical time
complexity of $O(2^N)$.  A branch-and-bound algorithm (BBA) has been
devised to reduce the number of generated nodes by cutting subtrees
which do not contribute to the current-best solution.  It relies on
the fundamental property that the RSS increases when variables are
deleted from a regression model, that is:
%
\begin{align*}
  S_1\subseteq S_2\Rightarrow\rss(S_1)\geq\rss(S_2)\text{.}
\end{align*}
%
A cutting test is employed to determine which parts of the DCA tree
are redundant: A new node $\drop(S,j)$ is generated only if
$\rss(S)<r_j$ ($j=k+1,\ldots,n-1$).  The quantity $\rss(S)$ is said to
be the \emph{bound} of the subtree rooted in $(S,k)$: no subset model
extracted from the subtree can have a lower RSS
\citep{gatu:j_comput_graph_stat:06}.  Note that the BBA is an
exhaustive algorithm, i.e. it provides the exact solution to the
problem~\eqref{eq:all_subsets}.

To further reduce the computational cost, the all-subsets regression
problem can be restricted to a range of submodel
sizes~\citep{hofmann:comput_stat_data_an:07}.  In this case, the
problem~\eqref{eq:all_subsets} is reformulated as
%
\begin{equation}
  \label{eq:all_subsets:subrange}
  %
  S^*_n=\argmin_{\card{S}=n}\rss(S)
  \quad\text{for}\quad n=n_\text{min},\dots,n_\text{max}, 
\end{equation}
%
where $n_\text{min}$ and $n_\text{max}$ are the subrange limits
($1\leq n_\text{min}\leq n_\text{max}\leq N$).  The search will span
only a part of the DCA regression tree.  Specifically, nodes $(S,k)$
are not computed if $\card{S}<n_\text{min}$ or $k\geq n_\text{max}$.

The size of subtrees rooted in the same level decreases exponentially
from left to right.  In order to encourage the pruning of large
subtrees by the BBA cutting test, the variables in a given node can be
ordered such that a child node will always have a larger RSS
(i.e. bound) than its right siblings.  This strategy can be applied in
nodes of arbitrary depth.  However, computing the bounds of the
variables implies a computational overhead.  Thus, it is not advisable
to preorder the variables in all of the tree.  A parameter -- the
preordering radius $p$ -- has been introduced to control the extent
in which variable preordering takes
place~\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07}.
It accepts a value between $p=0$ (no preordering) and $p=N$
(preordering in all nodes); typically, $p=\lfloor N/3\rfloor$ produces
good performance results.

The computational efficiency of the BBA is improved by allowing the
algorithm to prune non-redundant portions of the regression tree.  The
so-called heuristic branch-and-bound algorithm (HBBA) relaxes the
cutting test by employing a set of tolerance parameters $\tau_n\ge 0$
($n=1,\ldots,N$), one for every submodel size.  A node $\drop(S,j)$ is
generated only if there exists at least one $i$ such that
%
\begin{equation}
  \label{eq:hbba}
  %
  (1+\tau_i)\cdot\rss(S)<r_i\text{,}
  \quad i=j,\ldots,n-1\text{.}
\end{equation}
%
The algorithm is non-exhaustive if $\tau_n>0$ for any $n$, meaning
that the computed solution is not guaranteed to be optimal.  The
algorithm cuts subtrees the more aggressively the greater the value of
$\tau_n$, thus increasing the computational efficiency.  The advantage
of the HBBA over other heuristic algorithms is that the relative error
of the solution is bounded by the tolerance
parameter~\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07},
thus giving the user control over the tradeoff between solution
quality and execution time.

The DCA and its derivatives report the $N$ subset models with the
lowest RSS, one for each subset size.  The user can then analyze the
list of returned subsets to determine the \dquote{best} subset, for
example by evaluating some criterion function.  This approach is
practical but not necessarily efficient.  The $f$-BBA specializes the
cutting test of the standard BBA for a particular criterion function
$f(S)=F(n,\rho)$, where $n=\card{S}$ and $\rho=\rss(S)$, under the
condition that the latter satisfies the monotonicity
property~\eqref{eq:f:monotonicity}.  Specifically, a node $\drop(S,j)$
is generated if and only if
%
\begin{equation*}
  F(j,\rss(S))<r_f\text{,}
\end{equation*}
%
where $r_f$ is the \emph{single} current-best solution.  This results
in a more \dquote{informed} cutting test, and in a smaller number of
generated nodes.



%--------------------------------------------------------------------%
% section:  Implementation in R                                      %
%--------------------------------------------------------------------%

\section[Implementation in R]{Implementation in \proglang{R}}
\label{sec:R}

The \proglang{R} package \pkg{lmSubsets} provides infrastructure
for solving both the all-subsets regression \eqref{eq:all_subsets} and the
best-subset selection \eqref{eq:best_subset} problem in least-squares
regression models. The corresponding high-level \SSS{} generics are
\fct{lmSubsets} and \fct{lmSelect}, respectively.

The interfaces of both methods are closely modeled after the \fct{lm} function
and implement \R{}'s standard \code{formula} interface: they can be
called with any entity that can be coerced to a \code{formula} object \citep{chambers:92}.
After the \code{formula} description has been processed, the raw data as
well as problem-specific parameters are forwarded to the appropriate
core function.  The workhorse functions \fct{lmSubsets\_fit} and
\fct{lmSelect\_fit} implement the core problem logic without any
\code{formula}-related overhead.  An \SSS{} object of class
\class{lmSubsets} or \class{lmSelect} is returned, for which several
standard extractor methods have been defined.  An overview of the
various functions is given in Table~\ref{tab:generators}.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Problem                & Function                 & Description                  \\
    \hline
    All-subsets regression & \fct{lmSubsets.default}  & Standard formula interface   \\
                           & \fct{lmSubsets\_fit}     & Workhorse function           \\
    \hline
    Best-subset regression & \fct{lmSelect.default}   & Standard formula interface   \\
                           & \fct{lmSelect.lmSubsets} & Coercion method              \\
                           & \fct{lmSelect\_fit}      & Workhorse function           \\
                           & \fct{lmSubsets\_select}  & Explicit conversion function \\
    \hline
  \end{tabular}
  \caption{Generator methods and functions.}
  \label{tab:generators}
\end{table}


%--------------------------------------------------------------------%
% section:  Specifying the problem                                   %
%--------------------------------------------------------------------%

\subsection{Specifying the regression and subset selection problem}
\label{sec:specifying}

The user first specifies a linear model and provides a dataset from
which numerical values for the variables are taken.  The initial model
is typically given in the form of a symbolic description.  A
\code{formula} object -- or any other object that can be coerced to a
\code{formula} -- specifies the dependent and independent variables,
which are taken from a \code{data.frame} object.  For example, the
call
%
\begin{Code}
  lmSubsets(mortality ~ precipitation + temperature1 + temperature7 +
    age + household + education + housing + population + noncauc +
    whitecollar + income + hydrocarbon + nox + so2 + humidity,
    data = AirPollution)
\end{Code}
%
specifies a response variable (\code{mortality}) and fifteen predictor
variables with all variables being taken from the \code{AirPollution}
dataset~\citep{miller:02}.  In this case, the call can be abbreviated
to
%
\begin{Code}
  lmSubsets(mortality ~ ., data = AirPollution)
\end{Code}
%
where the dot (\code{.}), as usual, stands for ``all variables not mentioned in
the left-hand side of the formula''.  By default, an intercept term is
included in the model; that is, the call in the previous example is
equivalent to
%
\begin{Code}
  lmSubsets(mortality ~ . + 1, data = AirPollution)
\end{Code}
%
To discard the intercept, the call may be rewritten as follows:
%
\begin{Code}
  lmSubsets(mortality ~ . - 1, data = AirPollution)
\end{Code}
%
During execution, candidate models can be rejected based on the
presence or absence of certain independent variables.  The parameter
\code{include} is employed to accept only submodels that comprise one
or more specified variables.  In the following example, only submodels
containing the variable \code{noncauc} will be retained:
%
\begin{Code}
  lmSubsets(mortality ~ ., include = "noncauc", data = AirPollution)
\end{Code}
%
Conversely, the \code{exclude} parameter can be employed to discard a
specific set of variables, as in the following example:
%
\begin{Code}
  lmSubsets(mortality ~ ., exclude = "whitecollar", data = AirPollution)
\end{Code}
%
The same effect can be achieved by rewriting the formula as follows:
%
\begin{Code}
  lmSubsets(mortality ~ . - whitecollar, data = AirPollution)
\end{Code}
%
The \code{include} and \code{exclude} parameters may be used in
combination, and both may specify more than one variable
(e.g., \code{include = c("noncauc", "whitecollar")}).

The criterion used for best-subset selection is evaluated following
the expression
%
\begin{equation*}
  -2\cdot\mathtt{logLik} + \mathtt{penalty}\cdot\mathtt{npar}\text{,}
\end{equation*}
%
where \code{penalty} is the \emph{penalty per model parameter} defined
in~\eqref{eq:aic}, \code{logLik} the log-likelihood of the fitted
model, and \code{npar} the number of model parameters (including the error variance).  The \code{penalty} value indicates
how strongly model parameters are penalized with higher values
favoring more parsimonious solutions.  When $\mathtt{penalty}=2$, the
criterion corresponds to Akaike's information criterion
\citep[AIC,][]{akaike:ieee_t_automat_contr:74}; when
$\mathtt{penalty}=\log(\mathtt{nobs})$, to Schwarz's Bayesian
information criterion \citep[BIC,][]{schwarz:ann_stat:78},
\texttt{nobs} being the number of observations.  For example, either
one of
%
\begin{Code}
  lmSelect(mortality ~ ., data = AirPollution, penalty = 2)
  lmSelect(mortality ~ ., data = AirPollution, penalty = "AIC")
\end{Code}
%
will select the best submodels according to the usual AIC.
By default, the \fct{lmSelect} function employs the BIC.


%--------------------------------------------------------------------%
% section:  Core functions                                           %
%--------------------------------------------------------------------%

\subsection{Core functions}
\label{sec:core}

The high-level interfaces first process the formula specification,
set up the model data, and pass these to dedicated core functions
-- along with further optional problem-specific arguments.
The core functions act as wrappers for the \proglang{C++} library
which implements the problem logic.  The full list of arguments (see
also Table~\ref{tab:params}) is given by:
%
\begin{verbatim}
  lmSubsets_fit(x, y, weights = NULL, offset = NULL,
    include = NULL, exclude = NULL, nmin = NULL,
    nmax = NULL, tolerance = 0, pradius = NULL, nbest = 1, ...,
    .algo = "phbba")

  lmSelect_fit(x, y, weights = NULL, offset = NULL,
    include = NULL, exclude = NULL, penalty = "BIC",
    tolerance = 0, pradius = NULL, nbest = 1, ...,
    .algo = "phbba")
\end{verbatim}
%
The model data is given by means of a numeric matrix \code{x}, and a
vector \code{y}.  The \code{weights} and \code{offset} parameters
correspond to the homonomic parameters of the \code{lm} function.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Parameter        & Description                                               & Representation                            \\
    \hline
    \code{x}         & Data matrix                                            & \code{numeric[nobs,nvar]}                    \\
    \code{y}         & Response variable                                      & \code{numeric[nobs]}                         \\
    \code{weights}   & Model weights                                          & \code{numeric[nobs]}                         \\
    \code{offset}    & Model offset                                           & \code{numeric[nvar]}                         \\
    \code{include}   & Regressors to force in                                 & \code{logical[nvar]}                         \\
    \code{exclude}   & Regressors to force out                                & \code{logical[nvar]}                         \\
    \code{nmin}      & Min.\ number of regressors \small(\fct{lmSubsets} only) & \code{integer[1]}                            \\
    \code{nmax}      & Max.\ number of regressors \small(\fct{lmSubsets} only) & \code{integer[1]}                            \\
    \code{tolerance} & BBA tolerance parameters                               & \code{numeric[nvar]} \small(\fct{lmSubsets}) \\
                     &                                                        & \code{numeric[1]} \small(\fct{lmSelect})     \\
    \code{pradius}   & Preordering radius                                     & \code{integer[1]}                            \\
    \code{nbest}     & Number of best subsets                                 & \code{integer[1]}                            \\
    \code{.algo}     & Algorithm to execute                                   & \code{character[1]}                          \\
    \hline
  \end{tabular}
  \caption{Core parameters.}
  \label{tab:params}
\end{table}

The \code{include} and \code{exclude} parameters allow the user to
specify variables that are to be included in, or excluded from all
candidate models.  They can be either logical vectors -- with each entry
corresponding to one variable -- or are automatically expanded to such if
given in the form of an integer vector (i.e., set of variable indices)
or character vector (i.e., set of variable names).

For a large number of variables (see Section~\ref{sec:benchmark}),
execution times may become intractable.  In order to speed up the
execution, either the search space can be reduced, or one may settle
for a non-exhaustive solution.  In the first approach, the user may
specify values for the \code{nmin} and \code{nmax} parameters as
defined in~\eqref{eq:all_subsets:subrange}, in which case submodels
with less than \code{nmin} or more than \code{nmax} variables are
discarded, effectively removing entire branches of the regression tree
from the search space.

In the second approach, expectations with respect to the solution
quality are lowered, i.e., non-optimal solutions are \emph{tolerated}.
This is indicated by passing a numeric value -- typically between $0$
and $1$ -- to the \code{tolerance} parameter, which will be used by
the HBBA cutting test~\eqref{eq:hbba} to prune the search tree.  The
solution produced by the algorithm satisfies the following
relationship:
%
\begin{equation*}
  f(S)\leq(1+\mathtt{tolerance})\cdot f(S^*)\mathrm{,}
\end{equation*}
%
where $S$ is the returned solution, $S^*$ the optimal (theoretical)
solution, and $f$ the value of a submodel (e.g. deviance, AIC).  The
\fct{lmSubsets\_fit} function accepts a vector of tolerances, with one
entry for each subset size.

The \code{nbest} parameter controls how many submodels (per subset size) are retained.
In the case of \fct{lmSubsets\_fit}, a two-dimensional result set is
constructed with \code{nbest} submodels for each subset size, while in
the case of \fct{lmSelect\_fit}, a one-dimensional sequence of
\code{nbest} submodels is handed back to the user.

The \code{pradius} parameter serves to specify the desired preordering
radius.  The algorithm employs a default value of
$\lfloor\mathtt{nvar}/3\rfloor$.  The need to set this parameter
directly should rarely arise; please refer to Section~\ref{sec:comput}
for further information.  The \code{.algo} parameter serves to specify
the computational algorithm to be employed.  This parameter is used
for testing purposes only and should never be set by the user.


%--------------------------------------------------------------------%
% section:  Extracting submodels                                     %
%--------------------------------------------------------------------%

\subsection{Extracting submodels}
\label{sec:extracting}

The user is handed back a result object that encapsulates the
solution to an all-subsets (class \class{lmSubsets}) or best-subset
(class \class{lmSelect}) selection problem.  An object of class
\class{lmSubsets} represents a two-dimensional
$\mathtt{nbest}\times\mathtt{nvar}$ set of submodels; an object of
class \class{lmSelect}, a linear sequence of \code{nbest} submodels.
Problem-specific information is stored alongside the selected
submodels.  Table~\ref{tab:components} summarizes the components of
the result object.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Component        & Description                                & Representation                                                \\
    \hline           
    \code{nobs}      & Number of observations                     & \code{integer[1]}                                             \\
    \code{nvar}      & Number of regressors                       & \code{integer[1]}                                             \\
    \code{weights}   & Weights used                               & \code{numeric[nobs]}                                          \\
    \code{offset}    & Offset used                                & \code{numeric[nobs]}                                          \\
    \code{intercept} & Intercept flag                             & \code{logical[1]}                                             \\
    \code{include}   & Regressors forced in                       & \code{logical[nvar]}                                          \\
    \code{exclude}   & Regressors forced out                      & \code{logical[nvar]}                                          \\
    \code{penalty}   & Penalty used \small(\class{lmSelect} only) & \code{numeric[nvar]}                                          \\
    \code{tolerance} & Tolerances used                            & \code{numeric[nvar]}                                          \\
    \code{nbest}     & Number of best subsets                     & \code{integer[1]}                                             \\
    \code{df}        & Degrees of freedom                         & \code{integer[nbest,nvar]} \small(for \class{lmSubsets})      \\
                     &                                            & \code{integer[nbest]} \small(for \class{lmSelect})            \\
    \code{rss}       & Residual sum of squares                    & \code{numeric[nbest,nvar]} \small(for \class{lmSubsets})      \\
                     &                                            & \code{numeric[nbest]} \small(for \class{lmSelect})            \\
    \code{which}     & Selected regressors                        & \code{logical[nvar,nbest,nvar]} \small(for \class{lmSubsets}) \\
                     &                                            & \code{logical[nvar,nbest]} \small(for \class{lmSelect})       \\
    \hline
  \end{tabular}
  \caption{Components of \class{lmSubsets} and \class{lmSelect} objects.}
  \label{tab:components}
\end{table}

A wide range of standard methods to visualize, summarize, and extract
information is provided (see Table~\ref{tab:methods}).  The
\fct{print}, \fct{plot}, and \fct{summary} methods present a compact
overview -- either textual or graphical -- of information gathered
on the selected submodels, which can aid the user in identifying
``good'' submodels.  The remaining extractor functions behave in the
usual way, and can be used to extract variable names, coefficients,
covariance matrices, fitted values, etc.

In order to specify the submodel on which information is to be
obtained, \class{lmSubsets} methods provide two parameters, namely
\code{size} and \code{best}, which specify the number of regressors
in, and the ranking of the desired model, respectively.  The user must
always indicate the desired \code{size}, while \code{best} defaults to
\code{1} if left unspecified.  For \class{lmSelect} methods, the
\code{size} parameter has no meaning and is not defined.  Furthermore,
methods that return scalar values -- i.e., \fct{deviance},
\fct{logLik}, \fct{AIC}, \fct{BIC} -- can extract more than one
submodel at a time if passed a numeric vector as an argument to either
\code{size} (e.g., \code{size = 5:10}) or \code{best} (e.g.,
\code{best = 1:3}).

\begin{table}[t!]
  \centering
  \begin{tabular}{ll}
    \hline
    Method               & Description                     \\
    \hline
    \fct{print}          & Print object                    \\
    \fct{plot}           & Plot RSS (and information criteria) \\
    \fct{image}          & Heatmap of selected regressors  \small(\class{lmSubsets} only) \\
    \fct{summary}        & Summary statistics              \\
    \hline
    \fct{variable.names} & Extract variables names         \\
    \fct{formula}        & Extract formula object          \\
    \fct{model.frame}    & Extract (full) model frame      \\
    \fct{model.matrix}   & Extract model matrix            \\
    \fct{model.response} & Extract model response          \\
    \fct{refit}          & Fit subset \class{lm} model     \\
    \fct{coef}           & Extract regression coefficients \\
    \fct{vcov}           & Extract covariance matrix       \\
    \fct{fitted}         & Extract fitted values           \\
    \fct{residuals}      & Extract residual values         \\
    \fct{deviance}       & Extract deviance (RSS)          \\
    \fct{logLik}         & Extract log-likelihood          \\
    \fct{AIC}            & Extract AIC values              \\
    \fct{BIC}            & Extract BIC values              \\
    \hline
  \end{tabular}
  \caption{\SSS{} methods for \class{lmSubsets} and \class{lmSelect}
    objects.}
  \label{tab:methods}
\end{table}


%--------------------------------------------------------------------%
% section:  Case study I                                             %
%--------------------------------------------------------------------%

\section{Case study: Variable selection for weather forecasting}
\label{sec:usecase:1}

Over the last decades the field of weather forecasting made steady and
substantial improvements especially through improvements in numerical
weather prediction (NWP) models \citep{bauer:nature:15}.  Starting
from \cite{glahn:j_appl_meteorol:72} the outputs from these
physically-based large-scale (typically global) NWP models is
statistically post-processed to correct small-scale biases and obtain
predictions for specific locations. Below we use such model output
statistics (MOS) to predict temperature at a specific station
(Innsbruck Airport, Austria) based on a wide range of NWP quantities
from the nearest NWP grid point. Variable subset selection is relevant
here because it is not obvious which quantities beyond the temperature
NWP forecasts should enter the MOS regression.

More specifically, we model 00UTC temperature observations (in degree
Celsius) based on the corresponding 24-hour reforecast ensemble means
from the Global Ensemble Forecast System \citep[GEFS,][]{hamill:b_am_meteorol_soc:13}
for meteorological station Innsbruck Airport (11120; 47.260, 11.357) from 2011-01-01 to
2015-12-31. The data frame \code{IbkTemperature} contains 1824 daily
observations/forecasts for the observed response, 36 NWP outputs, and
five deterministic time trend/season patterns that are available as
potential regressors.  The NWP variables include several temperature
quantities (in degree Kelvin, e.g., 2-meter, minimum, maximum, soil)
as well as several quantities capturing precipitation, wind, and
fluxes among others. See \code{?IbkTemperature} for more details. The
data from the NOAA (United States National Oceanic and Atmospheric
Administration) are obtained from
\url{http://www.esrl.noaa.gov/psd/forecasts/reforecast2/}
(reforecasts) and \url{http://www.ogimet.com/synops.phtml.en}
(observations), respectively.

To start the analysis the data from the \pkg{lmSubsets} package can be
loaded and for simplicity a couple of days with some missing values are
omitted.
<<data>>=
data("IbkTemperature", package = "lmSubsets")
IbkTemperature <- na.omit(IbkTemperature)
@
First, a simple climatological model for the temperature (\code{temp})
with a linear trend (\code{time}) and a harmonic seasonal pattern
(\code{sin}/\code{cos} for the annual and \code{sin2}/\code{cos2}
for the bi-annual frequencies).
<<m0>>=
m0 <- lm(temp ~ time + sin + cos + sin2 + cos2, data = IbkTemperature)
@
This model does not make use of any NWP outputs and is used as a basic
reference model against which the subsequent MOS models can be compared.
Second, the model is updated to a simple MOS by including the most
obvious direct model output -- 2-meter temperature (\code{t2m}) --
in addition to the season/trend regressors.
<<m1>>=
m1 <- update(m0, . ~ . + t2m)
@ A graphical comparison of the raw data and the fitted values from
both models is provided in Figure~\ref{fig:temp}. The corresponding
estimated coefficients (and standard errors) are shown in
Table~\ref{tab:mtable} (produced with \pkg{memisc}, \citealp{memisc}).
This shows that, not surprisingly, inclusion of the 2-meter
temperature leads to substantial (and highly significant)
improvements. The season/trend coefficients are dampened but remain
significant which means that not all seasonal temperature pattern at
Innsbruck Airport are resolved in the coarse NWP grid.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=0.75\textwidth}
<<fig-temp, fig=TRUE, echo=FALSE, height=5.5, width=7>>=
par(mar = c(4, 4, 1, 1))
plot(temp ~ time, data = IbkTemperature, type = "l", col = "darkgray",
  ylim = c(-15, 35), xlab = "Time", ylab = "Temperature (Celsius)")
lines(fitted(m1) ~ time, data = IbkTemperature, col = "darkred")
lines(fitted(m0) ~ time, data = IbkTemperature, col = "black", lwd = 2)
legend("topright", c("Observations", "Climatology (m0)", "MOS (m1)"),
  lwd = 2, col = c("darkgray", "black", "darkred"), bty = "n")
@
\caption{\label{fig:temp} Observed temperature at Innsbruck Airport (gray) and
  fitted values from the climatological model (\code{m0}, black) and the
  simple MOS (\code{m1}, red).}
\end{figure}

Subsequently, we now try to improve the MOS by not only including
the direct model output for 2-meter temperature but also further NWP
model outputs. As a starting point we force the regressors from
\code{m1} into the model and use all-subsets regression to select
the relevant regressors from the remaining 35 NWP variables.
<<ms2>>=
ms2 <- lmSubsets(temp ~ ., data = IbkTemperature,
  include = c("t2m", "time", "sin", "cos", "sin2", "cos2"))
m2 <- refit(lmSelect(ms2, penalty = "BIC"))
@
After obtaining the all-subsets regression \code{ms2} with \fct{lmSubsets}
the best BIC solution is extracted with \fct{lmSelect} and turned
into a \class{lm} model with \fct{refit}. The more costly all-subsets
regression is solved here to gain more insights into the selected
variables not only for the best-BIC solution but also other models. 

To assess whether our initial MOS strategy in \code{m1} (and forced
into \code{m2}) is really the most suitable we also carry out another
all-subsets regression without restricting the search space.
<<ms3>>=
ms3 <- lmSubsets(temp ~ ., data = IbkTemperature)
m3 <- refit(lmSelect(ms3, penalty = "BIC"))
@
Obtaining \code{ms3} is computationally somewhat more costly than
\code{ms2} but still very fast taking only a couple of seconds on
standard PCs.

\begin{table}[t!]
\centering
<<mtable, echo=FALSE, results=tex>>=
library("memisc")
names(m2$coefficients) <- gsub("^x", "", names(m2$coefficients))
names(m3$coefficients) <- gsub("^x", "", names(m3$coefficients))
tab <- mtable(m0, m1, m2, m3, summary.stats = c("AIC", "BIC", "Deviance", "sigma", "R-squared"))
toLatex(tab)
@
\caption{\label{tab:mtable} Estimated regression coefficients (and standard errors)
along with further summary statistics for the climatological model \code{m0} and
the three MOS models (\code{m1}--\code{m3}).}
\end{table}

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<fig-rss, fig=TRUE, echo=FALSE, height=5.5, width=12>>=
par(mfrow = c(1, 2))
plot(summary(ms2))
plot(summary(ms3))
@
\caption{\label{fig:rss} Best RSS and associated BIC for all subset sizes considered
  in \code{ms2} (left) and \code{ms3} (right).}
\end{figure}

To assess the subset selections, Figure~\ref{fig:rss} shows a graphical summary
of best RSS and associated BIC for all subset sizes considered in \code{ms2}
and \code{ms3}, respectively. The plots can be easily produced by \code{plot(summary(...))}
and start with subset size 8 for \code{ms2} because seven variables are forced
into the model while in \code{ms3} only the intercept is always included. For
both models the RSS and BIC curves look rather similar and the best-BIC models
both have 13 regressors.

The corresponding selected variables can be seen in Table~\ref{tab:mtable}
produced by \code{mtable(m1, m2, m3, m4)}. This shows that both \code{m2}
and \code{m3} are rather similar with respect to the selected variables and
corresponding coefficients. However, interestingly the direct model output
\code{t2m} is not selected for \code{m3} and instead the soil temperature (\code{st})
as well as the maximal 2-meter temperature (\code{tmax2m}) and 
temperature on the so-called 2 PVU surface (\code{t2pvu}) are used which
are selected in addition to \code{t2m} in \code{m2}. Additionally, various
other meteorological quantities are selected that improve the forecasting model
further, e.g., soil moisture \code{vsmc}) among others.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<fig-image2, fig=TRUE, echo=FALSE, height=5, width=12>>=
par(mar = c(3.2, 4, 4, 1))
image(ms2, size = 8:20)
@
\caption{\label{fig:image2} Subset selection for \code{ms2}.}
\end{figure}

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<fig-image3, fig=TRUE, echo=FALSE, height=7, width=12>>=
par(mar = c(3.4, 4, 4, 1))
image(ms3, size = 2:20)
@
\caption{\label{fig:image3} Subset selection for \code{ms3}.}
\end{figure}

To gain further insight into the best-subset selection for various
sizes the \fct{image} method is useful. Figure~\ref{fig:image2} and
Figure~\ref{fig:image3} show the results for \code{ms2} and
\code{ms3}, respectively, for subset sizes up to 20 variables. The
dark cells in the heatmap show the variables that are selected while
the best-BIC solution is highlighted with a red rectangle and by
underlining the variable names in the x-axis labels. While there are
many similarities between the patterns shown, it is striking that
\code{t2m} is not considered for any of the subsets in \code{ms3}
while we forced it into \code{ms2}. The decision to always include the
deterministic season/trend regressors, however, is confirmed as some
of these regressors are already selected for low subset sizes and all
of them are selected from size 14 onwards.

Nevertheless, the differences between \code{m2} and \code{m3} in terms
of model fit are fairly small compared to the reference models \code{m0}
and \code{m1}. Comparing the BIC and the root mean squared error (RMSE)
gives
<<bic-rss>>=
BIC(m0, m1, m2, m3)
sqrt(sapply(list(m0, m1, m2, m3), deviance)/nrow(IbkTemperature))
@
showing that \code{m2} and \code{m3} improve the model substantially
over \code{m0} and \code{m1}. By construction the BIC of \code{m3}
is lower than that of \code{m2} but not by much.

In summary, this shows that best-subset selection can easily identify
relevant variables beyond the direct model output for MOS regressions,
yielding substantial improvements in forecasting quality. In a full
meteorological application this should, of course, be further tested
using cross-validation or other out-of-sample assessments. But as this
is beyond the scope of this paper we confine ourselves to the in-sample
assessment presented here.

To conclude, it is also worth pointing out that recently there has
been increasing interest in MOS models beyond least-squares linear
regression.  Incorporating heteroscedasticity is important ensemble
MOS models as well as censoring or truncation for quantities like
precipitation or wind (see the \pkg{crch} package of
\citealp{messner:r_j:16} for some examples). All-subsets regression
for those models would be much more burdensome and is not available in
\pkg{lmSubsets}. An alternative solution in those situations is for
example boosting as proposed by \cite{messner:16}.


%--------------------------------------------------------------------%
% section:  Case study II                                            %
%--------------------------------------------------------------------%

\section{Case study}
\label{sec:usecase:2}

\TODO{by Ana}


%--------------------------------------------------------------------%
% section:  Benchmark                                                %
%--------------------------------------------------------------------%

\section{Benchmark}
\label{sec:benchmark}

\TODO{Achim: Below are some rough ideas\dots Before going into detail
  about this one should check what other authors in this field have
  used for setting up their regressions.  Following their templates
  might make the design easier and more convincing for the reviewers.}

Data-generating process:
\begin{itemize}
  \item Allow increasing $M$ and/or $N$.
  \item A certain fixed fraction of variables could be relevant
    (say 20\%).
  \item All regressors could have the same distribution, e.g.,
    standard normal or uniform on $[-1, 1]$.
  \item The relevant variables could also all have the same
    coefficient or follow a linearly decreasing pattern for
    example. All irrelevant variables have coefficient zero.
  \item Then one could consider two values of $M$ (say, 200 and 1000)
    and a sequence of $N$s (say, 20, 40, \dots).
\end{itemize}

Competitors:
\begin{itemize}
  \item lmSubsets and lmSelect, either exactly or with certain
    relaxations.
  \item Other exact solutions (at least: leaps).
  \item Approximate solutions (at least: subselect and/or glmulti).
  \item Penalized solutions (at least: glmnet).
\end{itemize}

Outcome measurements:
\begin{itemize}
  \item Computation time of the R functions (especially for exact
    solutions).
  \item Proportion of correctly selected regressors.
  \item RSS, AIC, BIC.
\end{itemize}


%--------------------------------------------------------------------%
% section:  Acknowledgements                                         %
%--------------------------------------------------------------------%

\section*{Acknowledgements}

This work was in part supported by the \emph{F\"orderverein des
  wirtschaftswissenschaftlichen Zentrums der Universit\"at Basel},
through research project B-123.

The authors are grateful to Jakob Messner for sharing the GEFS
forecast data in \code{IbkTemperature}.


%====================================================================%
% BIBLIOGRAPHY                                                       %
%====================================================================%

\bibliography{lmSubsets}

\end{document}
