%====================================================================%
% PREAMBLE                                                           %
%====================================================================%

\documentclass[nojss]{jss}


%--------------------------------------------------------------------%
% vignette                                                           %
%--------------------------------------------------------------------%

% \VignetteIndexEntry{lmSubsets: Efficient Computation of Variable-Subsets Linear Regression in R}
% \VignetteKeywords{linear regression, model selection, variable selection, best-subset regression, R}
% \VignettePackage{lmSubsets}
% \VignetteDepends{stats,graphics}


%--------------------------------------------------------------------%
% Sweave options                                                     %
%--------------------------------------------------------------------%

%% pdflatex:  set 'eps=FALSE'
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}


%--------------------------------------------------------------------%
% packages                                                           %
%--------------------------------------------------------------------%

\usepackage{thumbpdf,lmodern}
\usepackage{pdfcomment}
\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-tree}
\usepackage{auto-pst-pdf}


%--------------------------------------------------------------------%
% commands                                                           %
%--------------------------------------------------------------------%

\newcommand{\Rset}{\mathbb{R}}
\newcommand{\rss}{\text{RSS}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\card}[1]{\lvert{#1}\rvert}
\newcommand{\SSS}{\proglang{S}3}
\newcommand{\R}{\proglang{R}}
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\class}[1]{\dquote{\code{#1}}}
%
\newcommand{\TODO}[1]{\texttt{TODO: #1}}


%--------------------------------------------------------------------%
% front matter                                                       %
%--------------------------------------------------------------------%

\author{%
  Marc Hofmann\\\TODO{affiliation}
  \And Cristian Gatu\\\TODO{affiliation}
  \And Erricos J. Kontoghiorghes\\Birbeck College
  \AND Ana Colubi\\University of Oviedo
  \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Marc Hofmann, Cristian Gatu, Erricos J. Kontoghiorghes, Ana Colubi, Achim Zeileis}

\title{\pkg{lmSubsets}: Efficient Computation of Variable-Subsets Linear Regression in \proglang{R}}
\Plaintitle{lmSubsets: Efficient Computation of Variable-Subsets Linear Regression in R}

\Abstract{ 
  \TODO{write abstract}
}

\Keywords{linear regression, model selection, variable selection, best subset regression, \proglang{R}}
\Plainkeywords{linear regression, model selection, variable selection, best subset regression, R}

\Address{
  Marc Hofmann\\
  \TODO{address}
}

%====================================================================%
% DOCUMENT                                                           %
%====================================================================%

\begin{document}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("lmSubsets")
data("AirPollution")
@


%--------------------------------------------------------------------%
% section:  Introduction                                             %
%--------------------------------------------------------------------%

\section{Introduction}
\label{sec:intro}

An important problem in statistical modeling is that of
subset-selection regression or, equivalently, of finding the best
regression equation~\citep{hastie:01}.  Given a set of possible
variables to be included in the regression, the problem is to select a
subset which optimizes some statistical criterion.  The latter
originates in the estimation of the corresponding
submodel~\citep{miller:02}.  Consider the standard regression model
%
\begin{equation}
  \label{eq:olm}
  %
  y=X\beta+\epsilon,\qquad\epsilon\sim(0,\sigma^2I_M),
\end{equation}
%
where $y\in\Rset^M$ is the output variable vector, $X\in
\Rset^{M\times N}$ is the exogenous data matrix of full column rank,
$\beta\in\Rset^N$ is the coefficient vector and $\epsilon\in\Rset^N$
is the noise vector.  The columns of $X$ correspond to the set of
exogenous variables $V = \{v_1,\dots,v_N \} $.  A submodel $S$
of~\eqref{eq:olm} comprises some of the variables in $V$. The
best-subset selection problem is to determine
%
\begin{equation}
  \label{eq:best-subsets}
  %
  S^*=\argmin_{S \subseteq V} f(S),\quad\text{where $f$ is some criterion function.}
\end{equation}
%
Standard selection criteria include the AIC family~\citep{miller:02}.
For constant number of parameters these criteria are monotonic
functions of the residual sum of squares (RSS), and attain their
optimal value when the RSS is minimal.  Thus~\eqref{eq:best-subsets}
can be re-written as a two stage problem.  In a first stage find
%
\begin{equation}
  \label{eq:all-subsets}
  %
  S^*_n=\argmin_{S \subseteq V,\ \card{S}=n}\rss(S)\quad\text{for}\quad n=1,\dots,N, 
\end{equation}
%
and in the second stage select
%
\begin{equation}
  \label{eq:all-subsets:selection}
  %
  S^*=\argmin_{S^*_n, n=1,\dots,N} f(S^*_n).
\end{equation}

Solving~\eqref{eq:all-subsets} means to compute all-subset models
corresponding to each submodel size.  Since the $ S^*_n$ ($ n=1,\dots,N $)
do not depend on a model size penalty, the solution
of~\eqref{eq:all-subsets} can rely on the \rss{} only.  In the second
stage the best-submodel $S^*$ can be selected with respect to any
standard criterion over the $N$ submodels $S^*_1,\dots,S^*_N$.
Therefore, solving the problem~\eqref{eq:best-subsets} is equivalent
in solving the problems~\eqref{eq:all-subsets}
and~\eqref{eq:all-subsets:selection}.  Note that solving directly the
best-subset problem~\eqref{eq:best-subsets} can allow a computational
strategy to focus on finding the best submodel at lower computational
cost.  On the other hand solving the problem~\eqref{eq:all-subsets}
can require a higher computational effort.  Still, in this case the
output will be a list of $N$ submodels on which a further
selection/analysis can be performed.

Algorithms for subset regression that do exact search but avoid
exhaustive fitting of all models can be found on the \proglang{R}
package~\pkg{leaps}~\citep{leaps} based on \cite{miller:02}.
Exhaustive search has been considered also within the context of
generalized linear models and summarized on the \pkg{bestglm}
package~\citep{bestglm}.
%
Alternative approximate solutions based on simulated annealing were
introduced in the \pkg{subselect} package~\citep{subselect} based
on~\cite{duarte_silva:j_multivariate_anal:01}.  Furthermore
approximate solutions via genetic algorithms for generalized linear
models and beyond have been implemented in
\pkg{glmulti}~\citep{calcagno:j_stat_softw:10} and \pkg{kofnGA}
\citep{wolters:j_stat_softw:15}.  Regularized estimation of parametric
models with automatic variable selection is performed by lasso or
elastic net estimation for generalized linear
models~\citep{friedman:j_stat_softw:10}.

Here, the \pkg{lmSubsets} package for exact, best variable-subset
regression is presented. It implements the algorithms presented by
\cite{gatu:j_comput_graph_stat:06} and
\cite{hofmann:comput_stat_data_an:07}.  The package embeds functions
that solve both the best-subset problem~\eqref{eq:best-subsets} and
the all-subsets problem~\eqref{eq:all-subsets}.  A branch-and-bound
device is employed to prune non-optimal sub-spaces when computing the
solution.  Exact and approximate strategies are deployed in order to
improve the computational performance of the branch-and-bound
algorithm.  The estimation numerical tool employed is the QR
decomposition and its modification.  Computationally intensive core
code is written in \proglang{C++}.  It is available as a
package~\citep{lmSubsets} for the \proglang{R} system for statistical
computing~\citep{R} from the Comprehensive \proglang{R} Archive
Network at \url{https://CRAN.R-project.org/package=lmSubsets}.


%--------------------------------------------------------------------%
% section:  Computational strategies                                 %
%--------------------------------------------------------------------%

\section{Computation strategies}
\label{sec:submodel}

In the linear regression model from Equation~\ref{eq:olm}
there are $2^N-1$ possible subset models and an exhaustive computation of
all of them is only feasible for small values of $N$. However, regression
trees can be employed to traverse the search space in a systematic fashion,
avoiding the need to explicitly compute every subset combination.
Figure~\ref{fig:dca_tree} illustrates a regression tree for $N=5$
variables.  A node in the regression tree is a pair $(S,k)$, where
$S=(s_1,\ldots,s_n)$ is a certain subset of $n$ variables and $k$ is
an integer, symbolized by a $\bullet$.

\begin{figure}[ht!]
  \def\node#1{\TR{\psframebox[framearc=0.5]{#1}}}
  \begin{center}
    \pstree[levelsep=8ex,treesep=2ex,edge=\ncline]{\node{$\bullet$12345}}{%
      \pstree{\node{$\bullet$2345}}{%
        \pstree{\node{$\bullet$345}}{%
          \pstree{\node{$\bullet$45}}{%
            \node{$\bullet$5}}%
          \node{3$\bullet$5}}%
        \pstree{\node{2$\bullet$45}}{%
          \node{2$\bullet$5}}%
        \node{23$\bullet$5}}%
      \pstree{\node{1$\bullet$345}}{%
        \pstree{\node{1$\bullet$45}}{%
          \node{1$\bullet$5}}%
        \node{13$\bullet$5}}%
      \pstree{\node{12$\bullet$45}}{%
        \node{12$\bullet$5}}%
      \node{123$\bullet$5}}
\end{center}
\caption{All-subsets regression tree for $N=5$ variables.}
\label{fig:dca_tree}
\end{figure}

Each node corresponds to a unique subset of variables, although not
every possible subset gives rise to a node.  Thus, the number of nodes
is $2^{N-1}$.  The root node corresponds to the full set of variables,
that is $(V, 1)$. When the algorithm visits node $(S,k)$, it reports
the {\rss} of the models corresponding to the leading variable subsets
of size $k+1$, \ldots, $n$, i.e., the subleading models
$(s_1,\ldots,s_{k+1})$, \ldots, $(s_1,\ldots,s_n)$.  It then generates
and in turn visits the nodes $(S-\{s_j\},j-1)$ for
$j=n-1,\ldots,k+1$.  The reported {\rss} is stored in a subset
table~$r$ along with its corresponding subset of variables.  The entry
$r_n$ corresponds to the RSS of the best subset model with~$n$
variables found so far~\citep{gatu:j_comput_graph_stat:06}.

The algorithm employs a branch-and-bound strategy to reduce the number
of generated nodes by cutting subtrees which do not contribute to the
best solution.  A cutting test is employed to determine which parts of
the tree are redundant.  That is, a new node $(S-\{s_j\},j-1)$ is
generated only if $\rss(S)<r_j$ ($j=k+1,\ldots,n-1$).  The quantity
$\rss(S)$ is said to be the \emph{bound} of the subtree rooted in
$(S,k)$; that is, no subset model extracted from the subtree can have
a lower RSS \citep{gatu:j_comput_graph_stat:06}.

The search can be restricted to find only the submodels of size within
a specified range.  This yields in spanning only a part of the
regression tree in Figure~\ref{fig:dca_tree}, and therefore it
requires a reduced computational cost.

In order to encourage the cutting of large subtrees, the regression
tree is generated such that large subtrees have greater bounds.  The
algorithm achieves this by preordering the variables.  Computing the
bounds of the variables implies a computational overhead.  Thus, it is
not advisable to preorder the variables in all the nodes.  A parameter
--- the preordering radius $p$ ($0\le p\le N$) --- defines the extent
to which variables are preordered
\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07}.

The efficiency of the branch-and-bound strategy is improved by
allowing the algorithm to prune non-redundant portions of the
regression tree.  Thus, the cutting test is relaxed by employing a
tolerance parameter $\tau_n\ge 0$ ($n=1,\ldots,N$).  A node
$(S-\{s_j\},j-1)$ is generated only if there exists at least one $i$
such that $(1+\tau_i)\cdot\rss(S)<r_i$ ($i=j,\ldots,n-1$).  The
algorithm is non-exhaustive if $\tau_n>0$ for any $n$, meaning that
the computed solution is not guaranteed to be optimal.  The algorithm
cuts subtrees more aggressively the greater the value of $\tau_n$; the
relative error of the solution is bounded by the employed tolerance
\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07}.

The algorithm reports the $N$ subset models with the lowest RSS, one
for each subset size.  The user can then analyze the list of returned
subsets to determine the \dquote{best} subset, e.g., by evaluating some
criterion function.  This approach is practical but not necessarily
efficient.  The algorithm may be optimized for a particular criterion
$f$ under the condition that the latter may be expressed as a function
of the subset size $n$ and the RSS $\rho$, i.e., $f(n,\rho)$, and that
$f$ is monotonic with respect to both $n$ and $\rho$. It takes a single
tolerance value and returns a single solution, that is the overall
(i.e., over all subset sizes) best subset model according to criterion
function $f$.


%--------------------------------------------------------------------%
% section:  Implementation in R                                      %
%--------------------------------------------------------------------%

\section[Implementation in R]{Implementation in \proglang{R}}
\label{sec:R}

Two \SSS{} generics are defined.  The \fct{lmSubsets} and
\fct{lmSelect} generator methods compute all-subsets and best-subset
regression, respectively.

The generator methods are closely modeled after the \fct{lm} function
and implement \R{}'s standard \code{formula} interface: they can be
called with any entity that can be coerced to a \code{formula} object.
After the \code{formula} instance has been resolved, the raw data as
well as problem-specific parameters are forwarded to the appropriate
core functions.  The workhorse functions \fct{lmSubsets\_fit} and
\fct{lmSelect\_fit} implement the core computational logic without any
\code{formula}-related overhead.  An \SSS{} object of class
\class{lmSubsets} or \class{lmSelect} is returned, for which several
standard extractor methods have been defined.  An overview of the
various functions is given in Table~\ref{tab:generators}.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Problem                 & Function                 & Description                  \\
    \hline
    All-subsets regression  & \fct{lmSubsets.default}  & Standard formula interface   \\
                            & \fct{lmSubsets\_fit}     & Workhorse function           \\
    \hline
    Best-subsets regression & \fct{lmSelect.default}   & Standard formula interface   \\
                            & \fct{lmSelect.lmSubsets} & Coercion method              \\
                            & \fct{lmSelect\_fit}      & Workhorse function           \\
                            & \fct{lmSubsets\_select}  & Explicit conversion function \\
    \hline
  \end{tabular}
  \caption{Generator methods and functions.}
  \label{tab:generators}
\end{table}


%--------------------------------------------------------------------%
% section:  Specifying the problem                                   %
%--------------------------------------------------------------------%

\subsection{Specifying the problem}
\label{sec:specifying}

The user first specifies a linear model and provides a dataset from
which numerical values for the variables are taken.  When
\emph{best}-subset regression is performed, a \emph{penalty} per
parameter must be indicated as well.

The initial model is typically given in the form of a symbolic
description.  A \code{formula} object --- or any other object that can
be coerced to a \code{formula} --- specifies the dependent and
independent variables, which are taken from a \code{data.frame}
object.  For example, the call
%
\begin{Code}
  lmSubsets(mortality ~ precipitation + temperature1 + temperature7 +
    age + household + education + housing + population + noncauc +
    whitecollar + income + hydrocarbon + nox + so2 + humidity,
    data = AirPollution)
\end{Code}
%
specifies a response variable (\code{mortality}) and fifteen predictor
variables; all variables are taken from the \code{AirPollution}
dataset~\citep{miller:02}.  In this case, the call can be abbreviated
to
%
\begin{Code}
  lmSubsets(mortality ~ ., data = AirPollution)
\end{Code}
%
where the dot (\code{.}) stands for ``all variables not mentioned in
the left-hand side of the formula''.  By default, an intercept term is
included in the model; that is, the call in the previous example is
equivalent to
%
\begin{Code}
  lmSubsets(mortality ~ . + 1, data = AirPollution)
\end{Code}
%
To discard the intercept, the call may be rewritten as follows:
%
\begin{Code}
  lmSubsets(mortality ~ . - 1, data = AirPollution)
\end{Code}
%
During execution, candidate models can be rejected based on the
presence or absence of certain independent variables.  The parameter
\code{include} is employed to accept only submodels that comprise one
or more specified variables.  In the following example, only submodels
containing the variable \code{noncauc} will be retained:
%
\begin{Code}
  lmSubsets(mortality ~ ., include = "noncauc", data = AirPollution)
\end{Code}
%
Conversely, the \code{exclude} parameter can be employed to discard a
specific set of variables, as in the following example:
%
\begin{Code}
  lmSubsets(mortality ~ ., exclude = "whitecollar", data = AirPollution)
\end{Code}
%
The same effect can be achieved by rewriting the formula as follows:
%
\begin{Code}
  lmSubsets(mortality ~ . - whitecollar, data = AirPollution)
\end{Code}
%
The \code{include} and \code{exclude} parameters may be used in
combination, and both may specify more than one variable
(e.g., \code{include = c("noncauc", "whitecollar")}).

The criterion used for best-subsets selection is evaluated following
the expression
%
\begin{equation*}
  -2\cdot\mathtt{logLik} + \mathtt{penalty}\cdot\mathtt{npar}\text{,}
\end{equation*}
%
where \code{penalty} is the \emph{penalty per model-parameter},
\code{logLik} the log-likelihood of the fitted model, and \code{npar}
the number of model-parameters.  The \code{penalty} value indicates
how strongly model parameters are penalized.  A higher \code{penalty}
will favor the selection of subset models with fewer regressors.  When
$\mathtt{penalty}=2$, the criterion corresponds to Akaike's
information criterion \citep[AIC,][]{akaike:ieee_t_automat_contr:74};
when $\mathtt{penalty}=\log(\mathtt{nobs})$, to Schwarz's Bayesian
information criterion \citep[BIC,][]{schwarz:ann_stat:78},
\texttt{nobs} being the number of observations.  For example,
%
\begin{Code}
  lmSelect(mortality ~ ., data = AirPollution, penalty = 2)
\end{Code}
%
will select the best submodels according to the usual AIC; or, in a
more idiomatic style:
%
\begin{Code}
  lmSelect(mortality ~ ., data = AirPollution, penalty = "AIC")
\end{Code}
%
By default, the \fct{lmSelect} function employs the BIC.


%--------------------------------------------------------------------%
% section:  Core functions                                           %
%--------------------------------------------------------------------%

\subsection{Core functions}
\label{sec:core}

The generator methods process the formula interface to extract the
model data, and pass the data --- along with any problem specific
arguments --- to the dedicated core functions.  The core functions act
as wrappers for the \proglang{C++} library which implements the
problem logic.  The full signature of the core functions is given by:
%
\begin{verbatim}
  lmSubsets_fit(x, y, weights = NULL, offset = NULL,
    include = NULL, exclude = NULL, nmin = NULL,
    nmax = NULL, tolerance = 0, pradius = NULL, nbest = 1, ...,
    .algo = "phbba")

  lmSelect_fit(x, y, weights = NULL, offset = NULL,
    include = NULL, exclude = NULL, penalty = "BIC",
    tolerance = 0, pradius = NULL, nbest = 1, ...,
    .algo = "phbba")
\end{verbatim}
%
The parameters are summarized in Table~\ref{tab:params}.

The model data is given by means of a numeric matrix \code{x} and
vector \code{y}.  The \code{weights} and \code{offset} parameters
correspond to the homonomic parameters of the \code{lm} function.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Parameter        & Description                                               & Representation                            \\
    \hline
    \code{x}         & Data matrix                                            & \code{numeric[nobs,nvar]}                    \\
    \code{y}         & Response variable                                      & \code{numeric[nobs]}                         \\
    \code{weights}   & Model weights                                          & \code{numeric[nobs]}                         \\
    \code{offset}    & Model offset                                           & \code{numeric[nvar]}                         \\
    \code{include}   & Regressors to force in                                 & \code{logical[nvar]}                         \\
    \code{exclude}   & Regressors to force out                                & \code{logical[nvar]}                         \\
    \code{nmin}      & Min.\ number of regressors \small(\fct{lmSubsets} only) & \code{integer[1]}                            \\
    \code{nmax}      & Max.\ number of regressors \small(\fct{lmSubsets} only) & \code{integer[1]}                            \\
    \code{tolerance} & BBA tolerance parameters                               & \code{numeric[nvar]} \small(\fct{lmSubsets}) \\
                     &                                                        & \code{numeric[1]} \small(\fct{lmSelect})     \\
    \code{pradius}   & Preordering radius                                     & \code{integer[1]}                            \\
    \code{nbest}     & Number of best subsets                                 & \code{integer[1]}                            \\
    \code{.algo}     & Algorithm to execute                                   & \code{character[1]}                          \\
    \hline
  \end{tabular}
  \caption{Core parameters.}
  \label{tab:params}
\end{table}

The \code{include} and \code{exclude} parameters allow the caller to
specify variables that are to be included in or excluded from all
candidate models.  They are logical vectors -- with each entry
corresponding to one variable -- and are automatically expanded if
given in the form of an integer vector (i.e., set of variable indices)
or character vector (i.e., set of variable names).

For a large number of variables (see Section~\ref{sec:benchmark}),
execution times may become intractable.  In order to speed up the
execution, either the search space can be reduced, or one may settle
for a non-exhaustive solution.  In the first appraoch, the caller can
specify values for the \code{nmin} and \code{nmax} parameters, in
which case submodels with less than \code{nmin} or more than
\code{nmax} variables are discarded, effectively removing entire
branches of the regression tree from the search space.

In the second approach, expectations with respect to the solution
quality are lowered, i.e., non-optimal solutions are \emph{tolerated}.
This is indicated by passing a numeric value --- typically between $0$
and $1$ --- to the \code{tolerance} parameter, which will be used by
the BBA cutting test to prune the search tree.  The solution produced
by the algorithm satisfies the following relationship:
%
\begin{equation*}
  f(S)\leq(1+\mathtt{tolerance})\cdot f(S^*)\mathrm{,}
\end{equation*}
%
where $S$ is the returned solution, $S^*$ the optimal (theoretical)
solution, and $f$ the value of a submodel (i.e., deviance, AIC).  The
\fct{lmSubsets\_fit} function accepts a vector of tolerances, with one
entry for each subset size.

The \code{nbest} parameter controls how many submodels are retained.
In the case of \fct{lmSubsets\_fit}, a two-dimensional result set is
constructed with \code{nbest} submodels for each subset size, while in
the case of \fct{lmSelect\_fit}, a one-dimensional sequence of
\code{nbest} submodels is handed back to the caller.

The \code{pradius} parameter serves to specify the desired preordering
radius.  The algorithm employs a default value of
$\lfloor\mathtt{nvar}/3\rfloor$.  The need to set this parameter
directly should rarely arise; please refer
to~\cite{hofmann:comput_stat_data_an:07} for further information.  The
\code{.algo} parameter serves to specify the computational algorithm
to be employed.  This parameter is used for testing purposes only and
should never be set directly.


%--------------------------------------------------------------------%
% section:  Extracting submodels                                     %
%--------------------------------------------------------------------%

\subsection{Extracting submodels}
\label{sec:extracting}

The caller is handed back a result object that encapsulates the
solution to an all-subsets (class \class{lmSubsets}) or best-subsets
(class \class{lmSelect}) selection problem.  An object of class
\class{lmSubsets} represents a two-dimensional
$\mathtt{nbest}\times\mathtt{nvar}$ set of submodels; an object of
class \class{lmSelect}, a linear sequence of \code{nbest} submodels.
Problem specific information is stored alongside the selected
submodels.  Table~\ref{tab:components} summarizes the components of
the result object.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Component        & Description                                & Representation                                                \\
    \hline           
    \code{nobs}      & Number of observations                     & \code{integer[1]}                                             \\
    \code{nvar}      & Number of regressors                       & \code{integer[1]}                                             \\
    \code{weights}   & Weights used                               & \code{numeric[nobs]}                                          \\
    \code{offset}    & Offset used                                & \code{numeric[nobs]}                                          \\
    \code{intercept} & Intercept flag                             & \code{logical[1]}                                             \\
    \code{include}   & Regressors forced in                       & \code{logical[nvar]}                                          \\
    \code{exclude}   & Regressors forced out                      & \code{logical[nvar]}                                          \\
    \code{penalty}   & Penalty used \small(\class{lmSelect} only) & \code{numeric[nvar]}                                          \\
    \code{tolerance} & Tolerances used                            & \code{numeric[nvar]}                                          \\
    \code{nbest}     & Number of best subsets                     & \code{integer[1]}                                             \\
    \code{df}        & Degrees of freedom                         & \code{integer[nbest,nvar]} \small(for \class{lmSubsets})      \\
                     &                                            & \code{integer[nbest]} \small(for \class{lmSelect})            \\
    \code{rss}       & Residual sum of squares                    & \code{numeric[nbest,nvar]} \small(for \class{lmSubsets})      \\
                     &                                            & \code{numeric[nbest]} \small(for \class{lmSelect})            \\
    \code{which}     & Selected regressors                        & \code{logical[nvar,nbest,nvar]} \small(for \class{lmSubsets}) \\
                     &                                            & \code{logical[nvar,nbest]} \small(for \class{lmSelect})       \\
    \hline
  \end{tabular}
  \caption{Components of \class{lmSubsets} and \class{lmSelect} objects.}
  \label{tab:components}
\end{table}

A wide range of standard methods to visualize, summarize, and extract
information is provided (see Table~\ref{tab:methods}).  The
\fct{print}, \fct{plot}, and \fct{summary} methods present a compact
overview --- either textual or graphical --- of information gathered
on the selected submodels, which the user can find useful to identify
``good'' models.  The remaining extractor functions behave in the
usual way, and can be used to extract variable names, coefficients,
covariances matrices, fitted values, etc.

In order to identify the submodel for which information is to be
extracted, \class{lmSubsets} methods provide two parameters, namely
\code{size} and \code{best}, which specify the number of regressors in
and the ranking of the desired model, respectively.  The \code{size}
argument is mandatory, while by default \code{best=1}.  For
\class{lmSelect} methods, the \code{size} parameter has no meaning and
is not defined.  Furthermore, methods that return scalar values ---
i.e., \fct{deviance}, \fct{logLik}, \fct{AIC}, \fct{BIC} --- can
process more than one submodel at a time, by passing a numeric vector
as an argument to either \code{size} (e.g., \code{size=5:10}) or
\code{best} (e.g., \code{best=1:3}).

\begin{table}[t!]
  \centering
  \begin{tabular}{ll}
    \hline
    Method               & Description                     \\
    \hline
    \fct{print}          & Print object                    \\
    \fct{plot}           & Plot object                     \\
    \fct{summary}        & Summary statistics              \\
    \hline
    \fct{variable.names} & Extract variables names         \\
    \fct{formula}        & Extract formula object          \\
    \fct{model.frame}    & Extract (full) model frame      \\
    \fct{model.matrix}   & Extract model matrix            \\
    \fct{model.response} & Extract model response          \\
    \fct{refit}          & Fit subset model                \\
    \fct{coef}           & Extract regression coefficients \\
    \fct{vcov}           & Extract covariance matrix       \\
    \fct{fitted}         & Extract fitted values           \\
    \fct{residuals}      & Extract residual values         \\
    \fct{deviance}       & Extract deviance (RSS)          \\
    \fct{logLik}         & Extract log-likelihood          \\
    \fct{AIC}            & Extract AIC values              \\
    \fct{BIC}            & Extract BIC values              \\
    \hline
  \end{tabular}
  \caption{\SSS{} methods for \class{lmSubsets} and \class{lmSelect}
    objects.}
  \label{tab:methods}
\end{table}


%--------------------------------------------------------------------%
% section:  Case study                                               %
%--------------------------------------------------------------------%

\section{Case study}
\label{sec:usecase}

\TODO{by Ana}


%--------------------------------------------------------------------%
% section:  Benchmark                                                %
%--------------------------------------------------------------------%

\section{Benchmark}
\label{sec:benchmark}

\TODO{by X}


%--------------------------------------------------------------------%
% section:  Acknowledgements                                         %
%--------------------------------------------------------------------%

\section*{Acknowledgments}

This work was in part supported by the \emph{F\"orderverein des
  wirtschaftswissenschaftlichen Zentrums der Universit\"at Basel},
through research project B-123.

%====================================================================%
% BIBLIOGRAPHY                                                       %
%====================================================================%

\bibliography{lmSubsets}

\end{document}
